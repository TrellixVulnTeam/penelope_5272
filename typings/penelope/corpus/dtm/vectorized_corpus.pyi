import numpy as np
import scipy
from ..document_index import DocumentIndex as DocumentIndex
from .convert import CoOccurrenceMixIn as CoOccurrenceMixIn
from .group import GroupByMixIn as GroupByMixIn
from .interface import IVectorizedCorpus as IVectorizedCorpus, VectorizedCorpusError as VectorizedCorpusError
from .slice import SliceMixIn as SliceMixIn
from .stats import StatsMixIn as StatsMixIn
from .store import StoreMixIn as StoreMixIn
from penelope import utility as utility
from typing import Any, Dict, Iterable, List, Mapping, Optional, Set, Tuple

logger: Any

class VectorizedCorpus(StoreMixIn, GroupByMixIn, SliceMixIn, StatsMixIn, CoOccurrenceMixIn, IVectorizedCorpus):
    def __init__(self, bag_term_matrix: scipy.sparse.csr_matrix, token2id: Dict[str, int], document_index: DocumentIndex, term_frequency_mapping: Dict[str, int]=...) -> None: ...
    @property
    def bag_term_matrix(self) -> scipy.sparse.csr_matrix: ...
    @property
    def token2id(self) -> Dict[str, int]: ...
    @property
    def id2token(self) -> Mapping[int, str]: ...
    @property
    def vocabulary(self): ...
    @property
    def T(self) -> scipy.sparse.csr_matrix: ...
    @property
    def term_frequency_mapping(self) -> Dict[str, int]: ...
    @property
    def term_frequencies(self) -> np.ndarray: ...
    @property
    def TF(self) -> np.ndarray: ...
    @property
    def document_token_counts(self) -> np.ndarray: ...
    @property
    def data(self) -> scipy.sparse.csr_matrix: ...
    @property
    def n_docs(self) -> int: ...
    @property
    def n_terms(self) -> int: ...
    @property
    def document_index(self) -> DocumentIndex: ...
    def todense(self) -> VectorizedCorpus: ...
    def get_word_vector(self, word: str) -> Any: ...
    def filter(self, px: Any) -> VectorizedCorpus: ...
    def normalize(self, axis: int=..., norm: str=..., keep_magnitude: bool=...) -> IVectorizedCorpus: ...
    def normalize_by_raw_counts(self): ...
    def year_range(self) -> Tuple[Optional[int], Optional[int]]: ...
    def xs_years(self) -> Tuple[int, int]: ...
    def token_indices(self, tokens: Iterable[str]) -> List[int]: ...
    def tf_idf(self, norm: str=..., use_idf: bool=..., smooth_idf: bool=...) -> IVectorizedCorpus: ...
    def to_bag_of_terms(self, indicies: Optional[Iterable[int]]=...) -> Iterable[Iterable[str]]: ...
    def co_occurrence_matrix(self) -> scipy.sparse.spmatrix: ...
    def find_matching_words(self, word_or_regexp: Set[str], n_max_count: int, descending: bool=...) -> List[str]: ...
    def find_matching_words_indices(self, word_or_regexp: List[str], n_max_count: int, descending: bool=...) -> List[int]: ...
    @staticmethod
    def create(bag_term_matrix: scipy.sparse.csr_matrix, token2id: Dict[str, int], document_index: DocumentIndex, term_frequency_mapping: Dict[str, int]=...) -> IVectorizedCorpus: ...

def find_matching_words_in_vocabulary(token2id: Mapping[str], candidate_words: Set[str]) -> Set[str]: ...
